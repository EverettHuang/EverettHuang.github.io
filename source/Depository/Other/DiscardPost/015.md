---
title: 监督学习算法：K邻近
date: 2019-10-18 14:22:03
tags: supervised learning algorithm
categories: machine learning
description: 本文介绍最简单的监督学习算法---K邻近
cover: /Depository/Img/Post/0015.png
top_img: /Depository/Img/Post/0013.jpg
---



**<center>k近邻(k-NN)</center>**

k近邻(k-NN)算法可以说是最简单的机器学习算法。**构建模型只需要保存训练数据集即可**。在对新数据作出预测时，算法会在数据集中找到最近的数据点，也就是它的“最近邻”

k-NN算法最简单的是考虑一个近邻的情况，也还可以考虑任意个(k个)邻居的情况，在考虑多个邻居时，采用投票法来指定标签

*算法对类别之间的分界线称为**决策边界**(decision boundary)*

#### Neighbors：

KNeighborsClassifier(用于分类), KNeighborsRegressor(用于回归)

`from sklearn.neighbors import KNeighborsClassifier KNeighborsRegressor`

**参数：**

* n_neighbors：n_neighbors参数表示选取的邻居个数
* 数据点之间截距的度量方法：默认使用欧式距离(很多情况下效果都很好)，其他度量本文不予讨论

**方法：**

* fit方法：调用该方法训练模型(该方法参数为训练集)
* predict方法：调用该方法预测标签(该方法参数为待预测标签的数据)
* score方法：调用该方法评估模型精度(参数为训练集时表示训练精度，参数为测试集时表示泛化精度)

*在KNeighborsRegressor模型时score方法返回R^2分数(决定系数)，是回归模型预测的优度度量，位于0和1之间，1对应完美预测*

#### k近邻分类

**KNeighborsClassifier**

`from sklearn.neighbors import KNeighborsClassifier`

```python
#  导入类并将其实例化
#  此处考虑3个邻居，n_neighbors参数为3
from sklearn.neighbors import KNeighborsClassifier
clf = KNeighborsClassifier(n_neighbors = 3)
#  利用训练集对这个分类器进行拟合(对KNeighborsClassifier来说就是保存数据集)
clf.fit(X_train, y_train)
#  调用predict方法对测试数据进行预测
clf.predict(X_test)
#  对测试数据和测试标签调用score方法评估模型泛化能力
clf.score(X_test, y_test)
```

#### k近邻回归

**KNeighborsRegressor**

`from sklearn.neighbors import KNeighborsRegressor`

```python
#  导入类并将其实例化，n_neighbors参数设置为3
from sklearn.neighbors import KNeighborsRegressor
reg = KNeighborsRegressor(n_neighbors = 3)
#  利用训练数据和训练目标值来拟合模型
reg.fit(X_train, y_train)
#  对测试集进行预测
reg.predict(X_test)
#  利用score方法评估模型泛化能力
reg.score(X_test, y_test)
```

#### k近邻优缺点

* 优点：k-NN模型易于理解，不需要过多调节就能获得不错性能，构建模型的速度通常很快

* 缺点：如果训练集很大，往往预测速度会很慢；对数据预处理很重要；对多特征(几百或更多)的数据效果不好，对大多数特征的大多数取值为0的数据集(稀疏数据集)效果尤其差

*虽然k-NN容易理解，但是由于预测速度慢且不能处理具有很多特征的数据集，所以时间中往往不会用到该算法*

---
