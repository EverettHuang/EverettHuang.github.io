---
title: 监督学习算法：线性模型
date: 2019-10-18 14:22:06
tags: supervised learning algorithm
categories: machine learning
description: 本文介绍监督学习算法中的线性模型，线性模型广泛运用于实践中
cover: /Depository/Img/Post/0016.jpg
top_img: /Depository/Img/Post/0013.jpg
---



**<center>线性模型</center>**

线性模型是在实践中广泛使用的一类模型

线性模型利用输入特征函数的**线性函数**(linear function)进行预测

**正则化**：对模型做显式约束，以避免过拟合
* L1正则化：L1正则化是指权值向量w中各个元素的绝对值之和，可以产生稀疏权值矩阵，即产生一个稀疏模型，可以用于特征选择
* L2正则化：L2正则化是指权值向量w中各个元素的平方和再求平方根，可以防止模型过拟合，一定程度上L1也可以防止模型过拟合

**线性模型方法**

* fit方法：调用该方法训练模型(该方法参数为训练集)
* predict方法：调用该方法预测标签(该方法参数为待预测标签的数据)
* score方法：调用该方法评估模型R^2(返回R^2，参数为训练集时表示训练精度,参数为测试集时表示泛化精度)

**线性模型属性**

* coef_属性：属性coef_保存“斜率”参数(w,也叫权重或系数)
* intercept_属性：属性intercept_保存偏移或截距(b)

*属性coef_为一个NumPy数组(每个元素对应一个输入特征)，属性intercept_为一个浮点数*

*属性后的下划线是因为scikit-learn为了与用户设置的参数区分开，总是将从训练数据中得出的值保存在以下划线结尾的属性中*

**线性模型主要参数**

* aplha(c)：在线性模型中最重要的参数为正则化参数，在回归模型中叫作alpha，在LinearSVC和LogisticRegression中叫做c；alpha值较大或c值较小，说明模型比较简单；假设只有几个特征是真正重要的，应该用L1正则化，否则默认使用L2正则化

---

#### 用于回归的线性模型

对于回归问题，线性模型预测的一般公式如下：

![线性模型预测的一般公式](URL "线性模型预测的一般公式")

![单一特征线性模型预测的一般公式](URL "单一特征线性模型预测的一般公式")

* x[0]到x[p]表示单个数据点的特征，w和b是学习模型的参数，y^是模型的预测结果

---

##### 线性回归(普通最小二乘法)

线性回归(普通最小二乘法,OLS)是回归问题最简单最经典的线性方法。

线性回归寻找参数w和b，使得对训练集的预测值与真实的回归目标值y之间的均方误差最小

*均方误差(mean squared erroe)：是预测值与真实值之差的平方和除以样本数*

**LinearRegression**

`form sklearn.linear_model import LinearRegression`

**参数**

*线性回归没有参数，这是一个优点，但也因此无法控制模型的复杂度*


```python
form sklearn.linear_model import LinearRegression
lr = LinearRegression().fit(X_train, y_train)
lr.coef_
lr.intercept_
lr.score(X_train, y_train)
lr.score(X_test, y_test)
```

线性模型对低维数据过拟合风险小，因为模型简单；但对高维数据(更多特征)线性模型非常强大，但是过拟合风险也大

---

##### 岭回归(Ridge)

岭回归也是一种用于回归的现行模型，因此它的预测公式与普通最小二乘法相同。岭回归用到L2正则化

*岭回归中，对系数(w)的选择不仅要在训练集上得到好的预测结果，而且还要拟合附加约束。系数尽量小，w的所有元素都应该接近于0，意味着每个特征对输出的影响尽可能小，同时仍给出很好的预测结果*

**Ridge**

`from sklearn.linear_model import Ridge`

**参数**

* alpha参数：默认参数alpha=1.0。增大alpha会使得系数更加趋近于0，从而降低训练集性能，但可能会提高泛化性能

```python
from sklearn.linear_model import Ridge
ridge = Ridge().fit(X_train, y_train)
ridge.score(X_train, y_train)
ridge.score(X_test, y_test)
```

*岭回归是一种约束性更强的模型，所以更不容易出现过拟合(训练数据足够多时，正则化变得不那么重要，性能都在增加，但线性回归性能将会追上岭回归)，应该选择Ridge而不是LinearRegression*

---

##### lasso

与岭回归相同，使用lasso也是约束系数使其接近于0，但是用的是L1正则化，使某些系数刚好为0(特征完全被模型忽略)

*使某些系数刚好为0(特征完全被模型忽略)，这样模型更容易解释，也可以呈现模型最重要的特征*

**Lasso**

`from sklearn.linear_model import Lasso`

**参数**

* alpha参数：默认参数alpha=1.0。增大alpha会使得系数更加趋近于0，从而降低训练集性能，但可能会提高泛化性能
* max_iter参数：运行迭代的最大次数。增加max_iter的值会增加模型复杂度，在欠拟合情况下增加

```python
from sklearn.linear_model import Lasso
lasso = Lasso().fit(X_train, y_train)
lasso = Lasso(alpha = 0.01, max_iter = 100000)
ridge.score(X_train, y_train)
ridge.score(X_test, y_test)
```

*在实践中，岭回归和Lasso首选岭回归。但如果特征很多，你认为只有其中几个是重要的，那么选择Lasso可能更好，且Lasso更容易解释。*

*scikit-learn中还提供了ElasticNet类，结合了Lasso和Ridge的惩罚项。在实践中这种结合最好，不过需要调节两个参数(一个用于L1正则化，一个用于L2正则化)*

---

##### ElasticNet

`sklearn.linear_model.ElasticNetCV`：可以通过迭代选择最佳的L1和L2(也可指定一组值)
`sklearn.linear_model.ElasticNet`：指定L1和L2

---

#### 用于分类的线性模型

许多的线性分类模型只适用于二分类问题，不能轻易推广到多类别问题(除了Logistic回归)

二分类线性模型公式

如果函数值小于0，就预测类别-1；如果函数大于0，就预测类别1；决策边界是输入的线性函数

将二分类推广到多分类算法的一种常见方法是“一对其余”方法：每个类别都学习一个二分类模型(有多少个类别就有多少个二分类模型)

置信方程

*置信方程结果中最大值对应的类别即为预测的类别标签*

*多分类Logistic回归背后的数学与“一对其余”方法稍显不同，但它也是对每个类别都有一个系数向量和一个截距，也使用了相同的预测方法*


最常见的两种线性分类算法是**Logistic回归**(logistic regression)和**线性支持向量机**(线性SVM)

**Logistic回归**

`from sklearn.linear_model import LogisticRegression`

**线性支持向量机**

`from sklearn.svm import LinearSVC`

两种模型都默认使用L2正则化

**参数**

* c参数：c = 1.0(c越大，对应的正则化越弱；较小的c可以让算法尽量适应“大多数”数据点，较大的c更强调每个数据点都分类的重要性)
* penalty参数：penalty = "l1"(可用penalty参数指定正则化方式)

---

#### 线性模型优缺点

* 优点：线性模型的训练速度和预测速度都很快，这种模型可以推广到非常大的数据集，稀疏数据有很有效；在包含十万甚至上百万个样本时，需要研究LogisticRegression和Ridge模型的solver = 'sag'选项，在处理大型数据时，这一选项比默认值更快；理解如何进行预测是相对容易的。

* 缺点：如果特征数量大于样本数量，线性模型的表现通常都很好，也常用于非常大的数据集，但在低维空间中，其他模型的泛化性能可能更好。

---
