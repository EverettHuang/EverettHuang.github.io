---
title: 监督学习算法：神经网络
date: 2019-10-18 14:22:18
tags: supervised learning algorithm
categories: machine learning
description: 本文简要介绍监督学习中的神经网络，且只介绍相对简单的多层感知机(MLP)
cover: /Depository/Img/Post/0022.jpeg
top_img: /Depository/Img/Post/0013.jpg
---

**<center>神经网络(深度学习)</center>**

*scikit-learn库不支持GPU(通常使用GPU可以加快10到100倍计算速度)，且还有更好的keras,lasagna和tensor-flow库(lasagna是基于theano库构建的，而keras既可以用tensor-flow也可以用theano)*

神经网络(深度学习)在许多机器学习应用中都有巨大的潜力，但深度学习算法往往经过精确调整，只适用于特定的使用场景

本文只介绍一些将对简单的方法，即用于分类和回归的多层感知机(MLP)，它可以作为研究更复杂的深度学习方法的起点。

*MLP也被成为(普通)前馈神经网络，有时也简称为神经网络*

---

**MLPClassifier**

`from sklearn.neural_network import MLPClassifier`

**方法**

* fit方法：调用该方法训练模型(该方法参数为训练集)
* predict方法：调用该方法预测标签(该方法参数为待预测标签的数据)
* score方法：调用该方法评估模型R^2(返回R^2，参数为训练集时表示训练精度,参数为测试集时表示泛化精度)

**参数**

* solver参数：该参数确定如何学习模型或用来学习参数的算法(该参数默认为adam，在大多数情况下都很好，但对数据缩放很敏感(均值为0,方差为1)；另一个值为lbfgs，其鲁棒性很好，但在大型模型或大型数据集上的时间会比较长；还有一个值为sgd，该值还有许多其他参数要调整，用以获取最佳结果(建议使用adam和lbfgs))
* hidden_layer_sizes参数：该参数确定使用几个隐层，每个隐层多少个隐结点,默认为100个隐结点(例如2个隐层，每个隐层10个隐结点：hidden_layer_sizes=[10,10])
* activation参数：该参数确定非线性，默认为relu，还可选择tanh非线性
* alpha参数：L2惩罚参数，默认值很小，与线性回归模型相同
* max_iter参数：该参数确定迭代次数(提示模型达到最大迭代次数后设置)

*神经网络调参的方法是：首先创建一个大到足以过拟合的网络，确保这个网络可以对任务进行学习，然后要么缩小网络，要么增大alpha参数来增强正则化，提高泛化性能*

**示例**

```python
#在two_moons数据集上训练MLP模型
from sklearn.neural_network import MLPClassifier
from sklearn.datasets import make_moons
X, y = make_moons(n_samples=100, noise=0.25, random_state=3)
X_train, y_train, X_test, y_test = train_test_split(X, y, stratify=y, random_state=42)
mlp = MLPClassifier(solver='lbfgs', random_state=0).fit(X_train, y_train)
#MLP默认使用100个隐结点(对于小型数据集已经很多了)
#减少隐结点数量(降低了模型复杂度)，但是仍能得到很好的结果
mlp = MLPClassifier(solver='lbfgs', random_state=0, hidden_layer_sizes=[10])
mlp.fit(X_train, y_train)
#使用2个隐层，每个包含10个单元
mlp = MLPClassifier(solver='lbfgs', random_state=0, hidden_layer_sizes=[10， 10])
mlp.fit(X_train, y_train)
#使用2个隐层，每个包含10个单元，使用tanh非线性(默认使用relu非线性)
mlp = MLPClassifier(solver='lbfgs', activation='tanh', random_state=0, hidden_layer_sizes=[10， 10])
mlp.fit(X_train, y_train)
#还可以利用L2惩罚使权重趋向于0，从而控制神经网络的复杂度，参数为alpha，默认数值很小(与线性模型中相同)
```

**优缺点**

* 优点：在机器学习的许多应用中，神经网络再次成为最先进的模型；能够获取大量数据中包含的信息，并且构建无比复杂的模型;给定足够的计算时间和数据，并且仔细调节参数，神经网络通常可以打败其他机器学习算法(无论是分类还是回归)

* 缺点：神经网络(特别是功能强大的大型神经网络)，通常需要很长的训练时间；需要仔细的预处理数据;与SVM类似，神经网络在“均匀”数据上性能最好(均匀是指所有特征都具有相似的含义；如果数据包含不同种类的特征，那么基于树的模型可能表现更好)；神经网络调参本身也是一门艺术

---


